#!/usr/bin/env node

// HeyGemini - Quick Gemini test via OpenWebUI Pipelines
// Updated for rEngine MCP integration architecture

import axios from 'axios';

import axios from 'axios';

// Try OpenWebUI direct endpoint first (fallback to Ollama)
const OPENWEBUI_BASE_URL = 'http://localhost:3031/ollama/api';
const BACKUP_PIPELINES_URL = 'http://localhost:9099/v1';
const OPENWEBUI_API_KEY = '0p3n-w3bu!';

async function testOpenWebUI(message) {
  try {
    console.log('ğŸ¤– Testing OpenWebUI connection...');
    console.log(`ğŸ“ Message: "${message}"`);
    console.log(`ğŸ”— Trying Ollama endpoint: ${OPENWEBUI_BASE_URL}`);
    
    // Try Ollama through OpenWebUI first
    const response = await axios.post(`${OPENWEBUI_BASE_URL}/chat`, {
      model: 'qwen2.5-coder:3b',
      messages: [
        { role: 'user', content: message }
      ],
      stream: false
    }, {
      timeout: 30000
    });

    console.log('âœ… Success via OpenWebUI Ollama!');
    console.log('ğŸ“¤ Response:');
    console.log(response.data.message.content);
    
  } catch (error) {
    console.log('âŒ OpenWebUI Ollama failed, trying pipelines...');
    
    try {
      // Fallback to pipelines
      const pipelineResponse = await axios.post(`${BACKUP_PIPELINES_URL}/chat/completions`, {
        model: 'qwen2.5-coder:3b',
        messages: [{ role: 'user', content: message }],
        max_tokens: 200
      }, {
        headers: {
          'Authorization': `Bearer ${OPENWEBUI_API_KEY}`,
          'Content-Type': 'application/json'
        },
        timeout: 15000
      });
      
      console.log('âœ… Success via Pipelines!');
      console.log('ğŸ“¤ Response:');
      console.log(pipelineResponse.data.choices[0].message.content);
      
    } catch (pipelineError) {
      console.log('âŒ Both endpoints failed:');
      console.log('ğŸ“‹ Ollama error:', error.message);
      console.log('ğŸ“‹ Pipelines error:', pipelineError.message);
      
      console.log('
ğŸ’¡ Status check:');
      console.log('  âœ… OpenWebUI container running (port 3031)');
      console.log('  âœ… Pipelines container running (port 9099)'); 
      console.log('  ğŸ”„ May need to configure models or wait for startup');
      console.log('
ğŸŒ™ Goodnight anyway! Tomorrow we'll get the full pipeline working! ï¿½');
      
      process.exit(1);
    }
  }
}

// Get message from command line or use default
const message = process.argv[2] || 'Hello! Testing OpenWebUI integration.';
testOpenWebUI(message); "${message}"`);
    console.log(`ğŸ¤– Model: ${model}`);
    console.log(`ğŸ”— Endpoint: ${OPENWEBUI_BASE_URL}`);
    
    const response = await axios.post(`${OPENWEBUI_BASE_URL}/chat/completions`, {
      model,
      messages: [
        { role: 'user', content: message }
      ],
      max_tokens: 1000,
      temperature: 0.7
    }, {
      headers: {
        'Authorization': `Bearer ${OPENWEBUI_API_KEY}`,
        'Content-Type': 'application/json'
      },
      timeout: 30000
    });

    console.log('âœ… Success!');
    console.log('ğŸ“¤ Response:');
    console.log(response.data.choices[0].message.content);
    
  } catch (error) {
    console.log('âŒ Error calling AI via OpenWebUI:');
    console.log('ğŸ“‹ Error details:', error.message);
    
    if (error.response) {
      console.log('ğŸ” Response status:', error.response.status);
      console.log('ğŸ” Response data:', error.response.data);
    }
    
    console.log('\nğŸ’¡ Troubleshooting:');
    console.log('  â€¢ Check if OpenWebUI Pipelines is running on port 9099');
    console.log('  â€¢ Verify the model is available');
    console.log('  â€¢ Test: curl http://localhost:9099/v1/models');
    
    process.exit(1);
  }
}

// Get message from command line or use default
const message = process.argv.slice(2).join(' ') || 'Hello! Testing OpenWebUI Pipelines integration.';
heyOpenWebUI(message);

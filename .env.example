# HexTrackr Environment Configuration
# Copy this file to .env and add your actual API keys

# Gemini API Configuration
GEMINI_API_KEY=your_gemini_api_key_here

# OpenAI API Configuration (also used by Memento for embeddings)
OPENAI_API_KEY=your_openai_api_key_here
OPENAI_EMBEDDING_MODEL=text-embedding-3-large

# Development Settings
NODE_ENV=development
DEBUG_MODE=false

# Documentation Generation Settings
DOCS_OUTPUT_DIR=docs-prototype/content
DOCS_TEMPLATE_DIR=docs-prototype/templates
AUTO_BACKUP_DOCS=true

# Security Settings
ENABLE_API_RATE_LIMITING=true
MAX_FILE_SIZE_MB=10
ALLOWED_FILE_EXTENSIONS=.js,.md,.json,.html,.css

# --- Memento MCP + Neo4j memory backend ---
# Choose memory storage type (neo4j recommended)
MEMORY_STORAGE_TYPE=neo4j

# Neo4j connection (matches docker-compose service)
NEO4J_URI=bolt://localhost:7687
NEO4J_USERNAME=neo4j
NEO4J_PASSWORD=neo4jpassword
NEO4J_DATABASE=memento

# Vector index settings
NEO4J_VECTOR_INDEX=memory_embedding_index
NEO4J_VECTOR_DIMENSIONS=1536
NEO4J_SIMILARITY_FUNCTION=cosine

# Debug logging for Memento
DEBUG=memento:*

# --- Ollama Local AI Configuration ---
# Ollama server configuration (auto-detected if running)
OLLAMA_BASE_URL=http://localhost:11434
OLLAMA_EMBEDDING_MODEL=nomic-embed-text:latest
OLLAMA_CHAT_MODEL=qwen2.5-coder:7b
OLLAMA_FALLBACK_MODEL=llama3.1:latest

# AI Provider Priority (ollama > anthropic > openai > local)
AI_PROVIDER_PRIORITY=ollama,anthropic,local

# --- Athena Memory System Configuration ---
# Athena processes Claude Code conversation logs for wisdom extraction
# Provider for embeddings: "openai" or "ollama" (default: ollama)
ATHENA_EMBEDDING_PROVIDER=ollama

# Athena file paths (relative to project root)
ATHENA_LOGS_DIR=logs
ATHENA_CLAUDE_LOGS_PATH=~/.claude/projects/-Volumes-DATA-GitHub-HexTrackr

# Embedding chunking configuration
ATHENA_CHUNK_SIZE=2048
ATHENA_CHUNK_OVERLAP=400

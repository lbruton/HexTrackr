# Vulnerability Import System - Data Model

## Entity Definitions

### Core Vulnerability Entity

```typescript
// Branded types for security-sensitive data
type CVEId = string & { readonly __brand: unique symbol };
type HostnameId = string & { readonly __brand: unique symbol };
type DeduplicationKey = string & { readonly __brand: unique symbol };
type UserId = string & { readonly __brand: unique symbol };

// Primary vulnerability record interface
interface VulnerabilityRecord {
    readonly id: string;                    // UUID v4 primary key
    readonly cve?: CVEId;                   // CVE identifier (validated format)
    readonly plugin_id?: string;           // Scanner-specific plugin ID
    readonly hostname: HostnameId;          // Affected host (validated FQDN/IP)
    readonly description: string;           // Sanitized vulnerability description
    readonly severity: VulnerabilitySeverity;
    readonly discovery_date: Date;          // Initial detection timestamp
    readonly last_seen: Date;              // Most recent detection
    readonly status: VulnerabilityStatus;
    readonly confidence_score: number;      // 0-100 deduplication confidence
    readonly metadata: VulnerabilityMetadata;
    readonly created_at: Date;
    readonly updated_at: Date;
}

// Enumerated types for type safety
enum VulnerabilitySeverity {
    CRITICAL = 'Critical',
    HIGH = 'High',
    MEDIUM = 'Medium',
    LOW = 'Low',
    INFO = 'Info'
}

enum VulnerabilityStatus {
    OPEN = 'Open',
    FIXED = 'Fixed',
    FALSE_POSITIVE = 'False Positive',
    ACCEPTED_RISK = 'Accepted Risk'
}

// Extended metadata with validation
interface VulnerabilityMetadata {
    readonly scanner_type: ScannerType;
    readonly original_severity?: string;     // Raw severity from scanner
    readonly cvss_score?: number;           // CVSS 2.0/3.x score if available
    readonly exploit_available?: boolean;    // Exploit code availability
    readonly patch_available?: boolean;      // Patch availability status
    readonly false_positive_risk?: number;  // ML-based false positive risk
    readonly raw_data: string;              // JSON string of original data
    readonly deduplication_method: DeduplicationMethod;
}

enum ScannerType {
    NESSUS = 'Nessus',
    OPENVAS = 'OpenVAS', 
    QUALYS = 'Qualys',
    CUSTOM = 'Custom'
}

enum DeduplicationMethod {
    CVE_BASED = 'CVE-Based',
    PLUGIN_HASH = 'Plugin-Hash',
    DESCRIPTION_FINGERPRINT = 'Description-Fingerprint'
}
```

### Import Batch Management

```typescript
interface ImportBatch {
    readonly id: string;                    // UUID v4 batch identifier
    readonly user_id: UserId;               // Reference to importing user
    readonly filename: string;              // Original uploaded filename
    readonly file_size: number;             // File size in bytes
    readonly file_hash: string;             // SHA-256 hash for integrity
    readonly status: ImportStatus;
    readonly total_records: number;
    readonly processed_records: number;
    readonly successful_imports: number;
    readonly failed_imports: number;
    readonly duplicate_records: number;
    readonly start_time: Date;
    readonly completion_time?: Date;
    readonly processing_time_ms?: number;
    readonly error_summary?: ImportErrorSummary;
    readonly performance_metrics: ImportMetrics;
}

enum ImportStatus {
    PENDING = 'Pending',
    PROCESSING = 'Processing', 
    COMPLETED = 'Completed',
    FAILED = 'Failed',
    CANCELLED = 'Cancelled'
}

interface ImportErrorSummary {
    readonly validation_errors: number;
    readonly duplicate_errors: number;
    readonly database_errors: number;
    readonly format_errors: number;
    readonly sample_errors: ImportError[];  // First 10 errors for diagnosis
}

interface ImportError {
    readonly line_number: number;
    readonly error_type: ErrorType;
    readonly error_message: string;
    readonly raw_data: string;
}

enum ErrorType {
    VALIDATION_ERROR = 'ValidationError',
    DUPLICATE_ERROR = 'DuplicateError', 
    FORMAT_ERROR = 'FormatError',
    DATABASE_ERROR = 'DatabaseError'
}

interface ImportMetrics {
    readonly memory_peak_mb: number;
    readonly records_per_second: number;
    readonly database_write_time_ms: number;
    readonly deduplication_time_ms: number;
    readonly validation_time_ms: number;
}
```

### Progress Tracking Entity

```typescript
interface ImportProgress {
    readonly batch_id: string;
    readonly current_phase: ProcessingPhase;
    readonly records_processed: number;
    readonly total_records: number;
    readonly progress_percentage: number;
    readonly estimated_completion: Date;
    readonly current_operation: string;
    readonly last_updated: Date;
}

enum ProcessingPhase {
    FILE_VALIDATION = 'File Validation',
    CSV_PARSING = 'CSV Parsing',
    DATA_VALIDATION = 'Data Validation',
    DEDUPLICATION = 'Deduplication',
    DATABASE_INSERT = 'Database Insert',
    AUDIT_LOGGING = 'Audit Logging',
    CLEANUP = 'Cleanup'
}
```

## Database Schema

### SQLite Configuration (WAL Mode Optimization)

```sql
-- Optimize SQLite for concurrent vulnerability processing
PRAGMA journal_mode = WAL;
PRAGMA synchronous = NORMAL;
PRAGMA cache_size = 65536;         -- 256MB cache
PRAGMA temp_store = MEMORY;
PRAGMA mmap_size = 1073741824;     -- 1GB memory mapping
PRAGMA foreign_keys = ON;
PRAGMA optimize;
```

### Staging Tables (Atomic Import Pattern)

```sql
-- Temporary staging table for atomic imports
CREATE TABLE vulnerability_staging (
    staging_id TEXT PRIMARY KEY,
    cve TEXT,
    plugin_id TEXT,
    hostname TEXT NOT NULL CHECK(LENGTH(hostname) <= 255),
    description TEXT NOT NULL CHECK(LENGTH(description) <= 4000),
    severity TEXT NOT NULL CHECK(severity IN ('Critical', 'High', 'Medium', 'Low', 'Info')),
    discovery_date TEXT NOT NULL,
    metadata TEXT NOT NULL CHECK(json_valid(metadata)),
    deduplication_key TEXT NOT NULL,
    confidence_score INTEGER CHECK(confidence_score >= 0 AND confidence_score <= 100),
    batch_id TEXT NOT NULL,
    created_at TEXT DEFAULT (datetime('now')),
    UNIQUE(deduplication_key, batch_id)
);

-- Optimized indexes for staging operations
CREATE INDEX idx_staging_dedup ON vulnerability_staging(deduplication_key);
CREATE INDEX idx_staging_batch ON vulnerability_staging(batch_id);
CREATE INDEX idx_staging_hostname ON vulnerability_staging(hostname);
CREATE INDEX idx_staging_severity_date ON vulnerability_staging(severity, discovery_date);
```

### Production Tables

```sql
-- Primary vulnerability storage with optimized schema
CREATE TABLE vulnerabilities_current (
    id TEXT PRIMARY KEY,
    cve TEXT,
    plugin_id TEXT,
    hostname TEXT NOT NULL CHECK(LENGTH(hostname) <= 255),
    description TEXT NOT NULL CHECK(LENGTH(description) <= 4000),
    severity TEXT NOT NULL CHECK(severity IN ('Critical', 'High', 'Medium', 'Low', 'Info')),
    status TEXT NOT NULL DEFAULT 'Open' CHECK(status IN ('Open', 'Fixed', 'False Positive', 'Accepted Risk')),
    discovery_date TEXT NOT NULL,
    last_seen TEXT NOT NULL,
    confidence_score INTEGER CHECK(confidence_score >= 0 AND confidence_score <= 100),
    metadata TEXT NOT NULL CHECK(json_valid(metadata)),
    created_at TEXT DEFAULT (datetime('now')),
    updated_at TEXT DEFAULT (datetime('now'))
);

-- Performance-optimized indexes based on query patterns
CREATE UNIQUE INDEX idx_vuln_dedup_key ON vulnerabilities_current(
    COALESCE(cve, '') || ':' || hostname || ':' || substr(description, 1, 100)
);
CREATE INDEX idx_vuln_hostname ON vulnerabilities_current(hostname);
CREATE INDEX idx_vuln_severity ON vulnerabilities_current(severity);
CREATE INDEX idx_vuln_last_seen ON vulnerabilities_current(last_seen);
CREATE INDEX idx_vuln_status ON vulnerabilities_current(status);
CREATE INDEX idx_vuln_cve ON vulnerabilities_current(cve) WHERE cve IS NOT NULL;
CREATE INDEX idx_vuln_severity_date ON vulnerabilities_current(severity, last_seen);

-- Historical snapshot storage for audit compliance
CREATE TABLE vulnerability_snapshots (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    vulnerability_id TEXT NOT NULL,
    operation TEXT NOT NULL CHECK(operation IN ('INSERT', 'UPDATE', 'DELETE')),
    previous_state TEXT,             -- JSON of previous record state
    new_state TEXT NOT NULL,        -- JSON of new record state
    changed_fields TEXT,             -- JSON array of changed field names
    batch_id TEXT,
    changed_by TEXT,
    changed_at TEXT DEFAULT (datetime('now')),
    FOREIGN KEY(vulnerability_id) REFERENCES vulnerabilities_current(id)
);

CREATE INDEX idx_snapshots_vuln_id ON vulnerability_snapshots(vulnerability_id);
CREATE INDEX idx_snapshots_date ON vulnerability_snapshots(changed_at);
CREATE INDEX idx_snapshots_batch ON vulnerability_snapshots(batch_id);

-- Import batch tracking with comprehensive metrics
CREATE TABLE import_batches (
    id TEXT PRIMARY KEY,
    user_id TEXT NOT NULL,
    filename TEXT NOT NULL,
    file_size INTEGER NOT NULL,
    file_hash TEXT NOT NULL,
    status TEXT NOT NULL CHECK(status IN ('Pending', 'Processing', 'Completed', 'Failed', 'Cancelled')),
    total_records INTEGER NOT NULL DEFAULT 0,
    processed_records INTEGER NOT NULL DEFAULT 0,
    successful_imports INTEGER NOT NULL DEFAULT 0,
    failed_imports INTEGER NOT NULL DEFAULT 0,
    duplicate_records INTEGER NOT NULL DEFAULT 0,
    start_time TEXT NOT NULL,
    completion_time TEXT,
    processing_time_ms INTEGER,
    error_details TEXT CHECK(json_valid(error_details)),
    performance_metrics TEXT CHECK(json_valid(performance_metrics)),
    created_at TEXT DEFAULT (datetime('now'))
);

CREATE INDEX idx_batches_user ON import_batches(user_id);
CREATE INDEX idx_batches_status ON import_batches(status);
CREATE INDEX idx_batches_date ON import_batches(start_time);

-- Daily aggregation for reporting performance
CREATE TABLE vulnerability_daily_totals (
    date TEXT PRIMARY KEY,
    total_vulnerabilities INTEGER DEFAULT 0,
    critical_count INTEGER DEFAULT 0,
    high_count INTEGER DEFAULT 0,
    medium_count INTEGER DEFAULT 0,
    low_count INTEGER DEFAULT 0,
    info_count INTEGER DEFAULT 0,
    new_today INTEGER DEFAULT 0,
    fixed_today INTEGER DEFAULT 0,
    updated_at TEXT DEFAULT (datetime('now'))
);

CREATE INDEX idx_daily_totals_date ON vulnerability_daily_totals(date);
```

## Validation Rules

### Input Validation Patterns

```typescript
// Compile-time validation with branded types
function validateCVE(input: string): CVEId {
    const cvePattern = /^CVE-\d{4}-\d{4,7}$/;
    if (!cvePattern.test(input)) {
        throw new ValidationError('Invalid CVE format');
    }
    return input as CVEId;
}

function validateHostname(input: string): HostnameId {
    // Combined FQDN and IPv4/IPv6 validation
    const hostnamePattern = /^(?:[a-zA-Z0-9](?:[a-zA-Z0-9-]{0,61}[a-zA-Z0-9])?\.)*[a-zA-Z0-9](?:[a-zA-Z0-9-]{0,61}[a-zA-Z0-9])?$|^(?:[0-9]{1,3}\.){3}[0-9]{1,3}$|^([0-9a-fA-F]{1,4}:){7}[0-9a-fA-F]{1,4}$/;
    
    if (!input || input.length > 255 || !hostnamePattern.test(input)) {
        throw new ValidationError('Invalid hostname format');
    }
    return input.toLowerCase() as HostnameId;
}

// SQL injection prevention for description fields
function sanitizeDescription(input: string): string {
    // Remove potential SQL injection patterns and limit length
    return input
        .replace(/[<>'"]/g, '') // Remove potential XSS characters
        .trim()
        .substring(0, 4000); // Enforce database constraint
}
```

### Business Logic Validation

```typescript
interface ValidationRule<T> {
    field: keyof T;
    validator: (value: any) => boolean;
    message: string;
}

const vulnerabilityValidationRules: ValidationRule<VulnerabilityRecord>[] = [
    {
        field: 'severity',
        validator: (value) => Object.values(VulnerabilitySeverity).includes(value),
        message: 'Severity must be Critical, High, Medium, Low, or Info'
    },
    {
        field: 'discovery_date',
        validator: (value) => value instanceof Date && value <= new Date(),
        message: 'Discovery date cannot be in the future'
    },
    {
        field: 'confidence_score',
        validator: (value) => Number.isInteger(value) && value >= 0 && value <= 100,
        message: 'Confidence score must be integer between 0-100'
    }
];
```

### Deduplication Logic

```typescript
// Secure deduplication key generation using hash functions
import { createHash } from 'crypto';

class SecureDeduplicationManager {
    generateDeduplicationKey(record: VulnerabilityRecord): DeduplicationKey {
        // Priority 1: CVE-based (100% confidence)
        if (record.cve) {
            const key = `CVE:${record.cve}:${record.hostname}`;
            return this.hashKey(key) as DeduplicationKey;
        }
        
        // Priority 2: Plugin ID with description hash (85% confidence)
        if (record.plugin_id) {
            const descHash = this.hashText(record.description);
            const key = `PLUGIN:${record.plugin_id}:${descHash}`;
            return this.hashKey(key) as DeduplicationKey;
        }
        
        // Priority 3: Description fingerprint (70% confidence)
        const fingerprint = this.generateDescriptionFingerprint(record.description);
        const key = `DESC:${record.hostname}:${fingerprint}`;
        return this.hashKey(key) as DeduplicationKey;
    }
    
    private hashKey(input: string): string {
        return createHash('sha256').update(input).digest('hex').substring(0, 16);
    }
    
    private hashText(text: string): string {
        return createHash('md5').update(text.toLowerCase().trim()).digest('hex').substring(0, 8);
    }
    
    private generateDescriptionFingerprint(description: string): string {
        // Extract key terms and create stable fingerprint
        const normalized = description
            .toLowerCase()
            .replace(/[^a-z0-9\s]/g, '')
            .split(/\s+/)
            .filter(word => word.length > 3)
            .sort()
            .slice(0, 5)
            .join('');
            
        return this.hashText(normalized);
    }
}
```

## Performance Constraints

### Transaction Management

```sql
-- Optimized batch processing with controlled transaction size
BEGIN IMMEDIATE TRANSACTION;

-- Insert staging data in chunks of 1000 records
INSERT INTO vulnerability_staging 
SELECT * FROM temp_import_data 
LIMIT 1000 OFFSET ?;

-- Validate and migrate to production with atomic operation
WITH validated_records AS (
    SELECT *, 
           CASE 
               WHEN cve IS NOT NULL THEN 100
               WHEN plugin_id IS NOT NULL THEN 85
               ELSE 70
           END as confidence_score
    FROM vulnerability_staging
    WHERE batch_id = ?
)
INSERT OR REPLACE INTO vulnerabilities_current
SELECT * FROM validated_records;

COMMIT TRANSACTION;
```

### Memory Management Constraints

- **Peak Memory Usage**: 512MB maximum during import
- **Chunk Processing**: 1,000-record batches for memory efficiency
- **Stream Processing**: Node.js readable streams for large files
- **Garbage Collection**: Explicit cleanup after each batch

### Performance Targets

- **Small Files** (<1K records): <5 seconds end-to-end
- **Medium Files** (1K-10K records): <30 seconds end-to-end
- **Large Files** (10K-100K records): <5 minutes end-to-end
- **Query Performance**: <500ms for vulnerability lookups
- **Concurrent Access**: Support 5 simultaneous reads during import

## Integration Mappings

### API Contract Integration

```typescript
// REST endpoint data transfer objects
interface VulnerabilityImportRequest {
    file: File;
    scanner_type: ScannerType;
    deduplicate: boolean;
    dry_run?: boolean;
}

interface VulnerabilityImportResponse {
    batch_id: string;
    status: ImportStatus;
    expected_records: number;
    websocket_url: string;
}

// WebSocket progress messaging
interface ProgressMessage {
    batch_id: string;
    phase: ProcessingPhase;
    progress_percentage: number;
    records_processed: number;
    total_records: number;
    estimated_completion: string;
    current_operation: string;
}
```

### Database Integration Patterns

```typescript
// Repository pattern for data access
interface VulnerabilityRepository {
    createBatch(batch: ImportBatch): Promise<void>;
    updateBatchProgress(batchId: string, progress: Partial<ImportBatch>): Promise<void>;
    stageVulnerabilities(records: VulnerabilityRecord[], batchId: string): Promise<void>;
    commitStagedRecords(batchId: string): Promise<number>;
    rollbackBatch(batchId: string): Promise<void>;
    createSnapshot(changes: VulnerabilitySnapshot[]): Promise<void>;
}

// Service layer integration
interface ImportService {
    processImport(request: VulnerabilityImportRequest, userId: UserId): Promise<VulnerabilityImportResponse>;
    getImportProgress(batchId: string): Promise<ImportProgress>;
    cancelImport(batchId: string, userId: UserId): Promise<void>;
}
```

## Migration Strategies

### Schema Evolution

```sql
-- Version 1.0 -> 1.1 Migration Example
ALTER TABLE vulnerabilities_current ADD COLUMN cvss_score REAL;
ALTER TABLE vulnerabilities_current ADD COLUMN exploit_available INTEGER DEFAULT 0;

-- Create new indexes for enhanced query performance
CREATE INDEX idx_vuln_cvss_score ON vulnerabilities_current(cvss_score) 
WHERE cvss_score IS NOT NULL;

-- Update existing records with migration script
UPDATE vulnerabilities_current 
SET cvss_score = json_extract(metadata, '$.cvss_score')
WHERE json_extract(metadata, '$.cvss_score') IS NOT NULL;
```

### Data Migration Procedures

1. **Backup Creation**: Automatic database backup before schema changes
2. **Incremental Updates**: Apply changes in small, reversible batches  
3. **Validation Testing**: Verify data integrity after each migration step
4. **Rollback Capability**: Maintain rollback scripts for each migration
5. **Performance Monitoring**: Track query performance before/after changes

---

*This data model supports the secure, high-performance implementation of HexTrackr Specification 002: Vulnerability Import System*

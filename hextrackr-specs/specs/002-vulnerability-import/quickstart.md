# Vulnerability Import System - Validation Guide

## Manual Validation Steps

### Phase 1: Environment Setup and Prerequisites

1. **Database Configuration Verification**

   ```sql
   -- Verify WAL mode is enabled for optimal performance
   PRAGMA journal_mode; -- Should return 'wal'
   PRAGMA synchronous;  -- Should return 1 (NORMAL)
   PRAGMA cache_size;   -- Should return 65536 (256MB)
   ```

2. **Service Dependencies Check**

   ```bash
   # Verify HexTrackr core services are running
   curl -f http://localhost:8989/api/health || echo "FAIL: Core service unavailable"
   
   # Check WebSocket server status
   netstat -an | grep :8989 || echo "FAIL: WebSocket port not listening"
   
   # Verify file upload directory permissions
   ls -la /tmp/hextrackr-uploads || mkdir -p /tmp/hextrackr-uploads
   ```

3. **Test Data Preparation**
   - **Small dataset**: Create test_small.csv (100 records, <1MB)
   - **Medium dataset**: Create test_medium.csv (5,000 records, ~10MB)  
   - **Large dataset**: Create test_large.csv (50,000 records, ~100MB)
   - **Edge cases**: Create test_edge_cases.csv with malformed data

### Phase 2: Core Import Functionality

4. **File Upload Validation**

   ```bash
   # Test 1: Valid CSV upload
   curl -X POST -F "file=@test_small.csv" -F "scanner_type=Nessus" \
        http://localhost:8989/api/vulnerabilities/import
   # Expected: 200 OK with batch_id and websocket_url
   
   # Test 2: File size limit enforcement (>100MB)
   curl -X POST -F "file=@oversized_file.csv" \
        http://localhost:8989/api/vulnerabilities/import
   # Expected: 413 Payload Too Large
   
   # Test 3: Invalid file type rejection
   curl -X POST -F "file=@test.txt" \
        http://localhost:8989/api/vulnerabilities/import  
   # Expected: 400 Bad Request with validation error
   ```

5. **CSV Format Validation**
   - **Required columns**: hostname, description, severity, discovery_date
   - **Optional columns**: cve, plugin_id, cvss_score
   - **Data validation**: Severity must be Critical|High|Medium|Low|Info
   - **Date validation**: ISO 8601 format (YYYY-MM-DD or YYYY-MM-DDTHH:mm:ss)

6. **Deduplication Logic Testing**

   ```javascript
   // Test data with known duplicates
   const testRecords = [
     { hostname: "server1.local", cve: "CVE-2024-0001", description: "Test vuln 1" },
     { hostname: "server1.local", cve: "CVE-2024-0001", description: "Test vuln 1" }, // Exact duplicate
     { hostname: "server1.local", plugin_id: "12345", description: "Different description" },
     { hostname: "server1.local", plugin_id: "12345", description: "Different description" } // Plugin duplicate
   ];
   
   // Expected results:
   // - CVE duplicates: 100% confidence merge
   // - Plugin duplicates: 85% confidence merge  
   // - Final count: 2 unique vulnerabilities
   ```

### Phase 3: Performance and Scalability Testing

7. **Import Performance Benchmarks**

   ```bash
   # Small dataset performance test (<5 seconds)
   time curl -X POST -F "file=@test_small.csv" \
             http://localhost:8989/api/vulnerabilities/import
   
   # Medium dataset performance test (<30 seconds)
   time curl -X POST -F "file=@test_medium.csv" \
             http://localhost:8989/api/vulnerabilities/import
   
   # Large dataset performance test (<5 minutes)
   time curl -X POST -F "file=@test_large.csv" \
             http://localhost:8989/api/vulnerabilities/import
   ```

8. **Memory Usage Monitoring**

   ```bash
   # Monitor memory usage during large import
   while true; do
     ps aux | grep "node.*hextrackr" | awk '{print $6}' | head -1
     sleep 5
   done
   # Expected: Peak memory <512MB regardless of file size
   ```

9. **Concurrent Import Testing**

   ```bash
   # Test multiple simultaneous imports (max 3 per user)
   curl -X POST -F "file=@test1.csv" http://localhost:8989/api/vulnerabilities/import &
   curl -X POST -F "file=@test2.csv" http://localhost:8989/api/vulnerabilities/import &  
   curl -X POST -F "file=@test3.csv" http://localhost:8989/api/vulnerabilities/import &
   curl -X POST -F "file=@test4.csv" http://localhost:8989/api/vulnerabilities/import &
   # Expected: First 3 succeed, 4th returns 429 Too Many Requests
   ```

### Phase 4: WebSocket Progress Tracking

10. **Real-time Progress Validation**

    ```javascript
    // Connect to WebSocket for import progress
    const ws = new WebSocket('ws://localhost:8989/progress/{batch_id}');
    
    ws.onmessage = (event) => {
      const progress = JSON.parse(event.data);
      console.log(`Phase: ${progress.phase}, Progress: ${progress.progress_percentage}%`);
      
      // Validate progress message structure
      assert(progress.batch_id !== undefined);
      assert(progress.phase !== undefined);  
      assert(progress.progress_percentage >= 0 && progress.progress_percentage <= 100);
      assert(progress.records_processed <= progress.total_records);
    };
    ```

## Automated Test Scenarios

### Unit Tests

```javascript
describe('Vulnerability Import Service', () => {
  describe('CSV Parsing', () => {
    it('should parse valid CSV with all columns', async () => {
      const csvData = 'hostname,cve,severity,description\nserver1.local,CVE-2024-0001,High,Test vulnerability';
      const result = await csvParser.parse(csvData);
      expect(result).toHaveLength(1);
      expect(result[0].hostname).toBe('server1.local');
    });
    
    it('should handle missing optional columns', async () => {
      const csvData = 'hostname,severity,description\nserver1.local,Critical,Test without CVE';
      const result = await csvParser.parse(csvData);
      expect(result[0].cve).toBeUndefined();
    });
    
    it('should reject invalid severity values', async () => {
      const csvData = 'hostname,severity,description\nserver1.local,Invalid,Test';
      await expect(csvParser.parse(csvData)).rejects.toThrow('Invalid severity');
    });
  });
  
  describe('Deduplication Engine', () => {
    it('should generate CVE-based keys with 100% confidence', () => {
      const record = { hostname: 'test.local', cve: 'CVE-2024-0001', description: 'Test' };
      const result = deduplicationEngine.generateKey(record);
      expect(result.confidence_score).toBe(100);
      expect(result.method).toBe('CVE-Based');
    });
    
    it('should generate plugin-based keys with 85% confidence', () => {
      const record = { hostname: 'test.local', plugin_id: '12345', description: 'Test' };
      const result = deduplicationEngine.generateKey(record);
      expect(result.confidence_score).toBe(85);
      expect(result.method).toBe('Plugin-Hash');
    });
    
    it('should prevent key collision attacks', () => {
      const maliciousRecord = { hostname: 'test.local', cve: 'CVE-2024-0001\'; DROP TABLE--', description: 'Attack' };
      const result = deduplicationEngine.generateKey(maliciousRecord);
      expect(result.key).not.toContain('DROP TABLE');
    });
  });
  
  describe('Staging Table Operations', () => {
    beforeEach(async () => {
      await database.query('BEGIN TRANSACTION');
    });
    
    afterEach(async () => {
      await database.query('ROLLBACK');
    });
    
    it('should create staging table with correct schema', async () => {
      const batchId = uuidv4();
      await stagingService.createStagingTable(batchId);
      
      const tableInfo = await database.query(`PRAGMA table_info(vulnerability_staging_${batchId})`);
      expect(tableInfo).toContainEqual(expect.objectContaining({ name: 'deduplication_key' }));
    });
    
    it('should enforce unique constraints in staging', async () => {
      const batchId = uuidv4();
      await stagingService.createStagingTable(batchId);
      
      const duplicateRecord = { deduplication_key: 'test123', hostname: 'server.local' };
      await stagingService.insertRecord(batchId, duplicateRecord);
      
      await expect(stagingService.insertRecord(batchId, duplicateRecord))
        .rejects.toThrow('UNIQUE constraint failed');
    });
  });
});
```

### Integration Tests

```javascript
describe('Import Integration Tests', () => {
  let testBatchId;
  
  beforeAll(async () => {
    // Setup test database and services
    await testSetup.initializeDatabase();
    await testSetup.startServices();
  });
  
  afterAll(async () => {
    await testSetup.cleanup();
  });
  
  it('should complete end-to-end import workflow', async () => {
    // Step 1: Submit import request
    const response = await request(app)
      .post('/api/vulnerabilities/import')
      .attach('file', 'test/fixtures/valid_vulnerabilities.csv')
      .field('scanner_type', 'Nessus')
      .expect(200);
    
    testBatchId = response.body.batch_id;
    
    // Step 2: Monitor progress via WebSocket
    const progressUpdates = await monitorImportProgress(testBatchId, 30000); // 30 second timeout
    expect(progressUpdates).toContainEqual(
      expect.objectContaining({ phase: 'Database Insert', progress_percentage: 100 })
    );
    
    // Step 3: Verify data in production tables
    const vulnerabilities = await database.query(
      'SELECT * FROM vulnerabilities_current WHERE batch_id = ?', [testBatchId]
    );
    expect(vulnerabilities.length).toBeGreaterThan(0);
    
    // Step 4: Verify audit trail
    const auditRecords = await database.query(
      'SELECT * FROM vulnerability_snapshots WHERE batch_id = ?', [testBatchId]
    );
    expect(auditRecords.length).toBe(vulnerabilities.length);
  });
  
  it('should handle import failures gracefully', async () => {
    const response = await request(app)
      .post('/api/vulnerabilities/import')
      .attach('file', 'test/fixtures/malformed_data.csv')
      .field('scanner_type', 'OpenVAS')
      .expect(200);
    
    const batchId = response.body.batch_id;
    
    // Wait for processing to complete
    await waitForBatchCompletion(batchId);
    
    // Verify batch status is 'Failed'
    const batch = await database.query('SELECT * FROM import_batches WHERE id = ?', [batchId]);
    expect(batch[0].status).toBe('Failed');
    
    // Verify no partial data in production tables
    const vulnerabilities = await database.query(
      'SELECT * FROM vulnerabilities_current WHERE batch_id = ?', [batchId]
    );
    expect(vulnerabilities.length).toBe(0);
    
    // Verify staging table cleanup
    const stagingTables = await database.query(
      `SELECT name FROM sqlite_master WHERE type='table' AND name LIKE 'vulnerability_staging_%'`
    );
    expect(stagingTables.length).toBe(0);
  });
});
```

### Performance Tests

```javascript
describe('Performance Tests', () => {
  it('should meet small dataset performance target', async () => {
    const startTime = Date.now();
    
    const response = await request(app)
      .post('/api/vulnerabilities/import')
      .attach('file', generateTestCSV(100)) // 100 records
      .field('scanner_type', 'Qualys');
    
    await waitForBatchCompletion(response.body.batch_id);
    
    const endTime = Date.now();
    const duration = endTime - startTime;
    
    expect(duration).toBeLessThan(5000); // <5 seconds
  });
  
  it('should maintain memory limits during large imports', async () => {
    const memoryMonitor = new MemoryMonitor();
    memoryMonitor.start();
    
    const response = await request(app)
      .post('/api/vulnerabilities/import')
      .attach('file', generateTestCSV(50000)) // 50k records
      .field('scanner_type', 'Custom');
    
    await waitForBatchCompletion(response.body.batch_id);
    
    const peakMemory = memoryMonitor.getPeakUsage();
    memoryMonitor.stop();
    
    expect(peakMemory).toBeLessThan(512 * 1024 * 1024); // <512MB
  });
  
  it('should handle concurrent imports without degradation', async () => {
    const concurrentImports = Array.from({ length: 3 }, (_, i) => 
      request(app)
        .post('/api/vulnerabilities/import')
        .attach('file', generateTestCSV(1000, `concurrent_${i}`))
        .field('scanner_type', 'Nessus')
    );
    
    const startTime = Date.now();
    const responses = await Promise.all(concurrentImports);
    
    // Wait for all imports to complete
    await Promise.all(responses.map(r => waitForBatchCompletion(r.body.batch_id)));
    
    const endTime = Date.now();
    const totalDuration = endTime - startTime;
    
    // Should not take more than 2x the single import time
    expect(totalDuration).toBeLessThan(20000); // <20 seconds for 3 concurrent
  });
});
```

## Common Issues & Solutions

### Issue 1: Import Timeout on Large Files

**Symptoms**: Import fails with timeout error after 2 minutes
**Cause**: Default request timeout too low for large datasets
**Solution**:

```javascript
// Increase timeout in Express configuration
app.use('/api/vulnerabilities/import', (req, res, next) => {
  req.setTimeout(600000); // 10 minutes
  res.setTimeout(600000);
  next();
});
```

### Issue 2: Memory Exhaustion During Import

**Symptoms**: Node.js process crashes with "JavaScript heap out of memory"
**Cause**: Loading entire CSV into memory at once
**Solution**:

```javascript
// Use streaming CSV parser instead of loading entire file
const csvStream = csv()
  .on('data', (row) => {
    processRecord(row);
    if (++recordCount % 1000 === 0) {
      // Pause stream to allow garbage collection
      csvStream.pause();
      setImmediate(() => csvStream.resume());
    }
  });
```

### Issue 3: Duplicate Detection False Positives

**Symptoms**: Different vulnerabilities being merged incorrectly
**Cause**: Description-based fingerprinting too aggressive
**Solution**:

```javascript
// Increase specificity of description fingerprint
function generateDescriptionFingerprint(description) {
  const normalized = description
    .toLowerCase()
    .replace(/[^a-z0-9\s]/g, '')
    .split(/\s+/)
    .filter(word => word.length > 4) // Increase minimum word length
    .sort()
    .slice(0, 8) // Include more words
    .join('');
  
  return hashText(normalized);
}
```

### Issue 4: WebSocket Connection Lost During Import

**Symptoms**: Progress updates stop mid-import
**Cause**: WebSocket connection timeout or network issues
**Solution**:

```javascript
// Implement WebSocket reconnection with exponential backoff
class ReconnectingWebSocket {
  constructor(url) {
    this.url = url;
    this.reconnectAttempts = 0;
    this.maxReconnectAttempts = 10;
    this.reconnectDelay = 1000; // Start with 1 second
    this.connect();
  }
  
  connect() {
    this.ws = new WebSocket(this.url);
    
    this.ws.onclose = () => {
      if (this.reconnectAttempts < this.maxReconnectAttempts) {
        setTimeout(() => {
          this.reconnectAttempts++;
          this.reconnectDelay *= 2; // Exponential backoff
          this.connect();
        }, this.reconnectDelay);
      }
    };
    
    this.ws.onopen = () => {
      this.reconnectAttempts = 0;
      this.reconnectDelay = 1000;
    };
  }
}
```

### Issue 5: Database Lock Contention

**Symptoms**: "database is locked" errors during import
**Cause**: Long-running transactions blocking other operations
**Solution**:

```sql
-- Implement smaller transaction batches
BEGIN IMMEDIATE TRANSACTION;
-- Process only 1000 records per transaction
INSERT INTO vulnerabilities_current 
SELECT * FROM vulnerability_staging 
LIMIT 1000 OFFSET ?;
COMMIT;

-- Add retry logic for lock conflicts
const MAX_RETRIES = 3;
let retries = 0;
while (retries < MAX_RETRIES) {
  try {
    await database.query(sql, params);
    break;
  } catch (error) {
    if (error.code === 'SQLITE_BUSY' && retries < MAX_RETRIES) {
      retries++;
      await sleep(Math.pow(2, retries) * 100); // Exponential backoff
    } else {
      throw error;
    }
  }
}
```

### Issue 6: CSV Parsing Encoding Issues

**Symptoms**: Special characters appearing as squares or question marks
**Cause**: Incorrect character encoding detection
**Solution**:

```javascript
// Detect and handle multiple encodings
const iconv = require('iconv-lite');
const jschardet = require('jschardet');

function parseCSVWithEncoding(buffer) {
  const detected = jschardet.detect(buffer);
  const encoding = detected.encoding || 'utf8';
  
  let csvContent;
  if (encoding.toLowerCase() !== 'utf8') {
    csvContent = iconv.decode(buffer, encoding);
  } else {
    csvContent = buffer.toString('utf8');
  }
  
  return csvContent;
}
```

## Success Criteria

### Functional Success Criteria

✅ **Import Completion**: All valid records successfully imported

- Small datasets (100 records): 100% success rate
- Medium datasets (5,000 records): >99% success rate  
- Large datasets (50,000 records): >98% success rate

✅ **Deduplication Accuracy**: Duplicate detection within acceptable ranges

- CVE-based matching: >99% accuracy
- Plugin-based matching: >95% accuracy
- Description-based matching: >85% accuracy (with <5% false positives)

✅ **Data Integrity**: All imported data maintains consistency

- Zero data corruption incidents
- Complete audit trail for all changes
- Referential integrity maintained across all tables

### Performance Success Criteria

✅ **Response Time Targets**:

- Small datasets: <5 seconds end-to-end
- Medium datasets: <30 seconds end-to-end
- Large datasets: <5 minutes end-to-end
- API response time: <2 seconds for upload acknowledgment

✅ **Resource Utilization**:

- Peak memory usage: <512MB during any import
- CPU utilization: <80% average during processing
- Database query time: <500ms for vulnerability lookups
- Concurrent import support: 3 simultaneous imports per user

✅ **Scalability Metrics**:

- Processing rate: >100 records/second for medium datasets
- Memory efficiency: <1KB per record peak memory usage
- Database growth: <10% storage overhead from staging operations

### Security Success Criteria  

✅ **Input Validation**: All malicious inputs rejected

- SQL injection attempts: 0% success rate
- XSS payloads: 100% sanitization
- File upload attacks: 100% detection and blocking
- Malformed data: Graceful error handling without system compromise

✅ **Access Control**: Security boundaries properly enforced

- Authentication required: 100% of import requests
- Role-based authorization: Proper enforcement for all operations
- Audit logging: 100% coverage of security-relevant events
- Session management: Secure session handling throughout import process

✅ **Data Protection**: Sensitive information properly secured

- Data in transit: HTTPS encryption for all API calls
- WebSocket security: Proper authentication for progress updates  
- Audit trail: Complete logging without exposing sensitive data
- Error messages: No sensitive information leaked in error responses

---

*This validation guide ensures comprehensive testing and verification of the HexTrackr Vulnerability Import System across all functional, performance, and security dimensions.*

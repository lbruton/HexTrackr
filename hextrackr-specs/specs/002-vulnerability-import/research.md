# Vulnerability Import System - Technical Research

## Architecture Decisions

### Primary Approach: Staging Table Pattern

**Decision:** Implement a two-phase import process using temporary staging tables followed by atomic data migration.

**Rationale:**

- Proven 99%+ performance improvement in existing HexTrackr implementations (8,022ms → 65ms for 12,542 records)
- Atomic operations prevent partial imports during failures
- Enables pre-validation of entire dataset before committing to production tables
- Supports rollback capabilities through staging table retention

**Alternatives Considered:**

- **Direct Insert Approach**: Rejected due to inability to validate entire dataset before commit
- **Stream Processing**: Rejected due to complexity and lack of atomic rollback capability
- **Batch Insert with Transactions**: Rejected due to memory constraints on large datasets

**Technology Stack:**

- **CSV Parser**: Papa Parse 5.x for robust CSV handling with streaming support
- **Database Operations**: SQLite with WAL mode for concurrent read/write operations
- **Progress Reporting**: WebSocket (socket.io 4.x) for real-time progress updates
- **File Processing**: Node.js Streams API for memory-efficient large file handling

### Deduplication Strategy

**Decision:** Multi-tiered unique key generation with confidence scoring.

**Primary Method**: CVE-based matching (highest confidence)

```javascript
function generatePrimaryKey(row) {
    if (row.cve && row.cve !== 'N/A') {
        return `CVE:${row.cve}:${row.hostname}`;
    }
    return null; // Fall to secondary
}
```

**Secondary Method**: Plugin ID with description hash

```javascript
function generateSecondaryKey(row) {
    if (row.plugin_id) {
        const descHash = hashFunction(row.description);
        return `PLUGIN:${row.plugin_id}:${descHash.substring(0,8)}`;
    }
    return null; // Fall to tertiary
}
```

**Tertiary Method**: Description-based fingerprinting with host correlation

## Integration Analysis

### HexTrackr Dependencies

- **Database Schema**: Leverages existing `vulnerabilities_current` table structure
- **Progress System**: Integrates with existing WebSocket infrastructure on port 8989
- **File Upload**: Uses existing Express.js multipart handling with 100MB limit
- **Authentication**: Inherits session-based authentication from HexTrackr core

### External Dependencies

- **CSV Format Compatibility**: Supports Nessus, OpenVAS, Qualys, and custom CSV formats
- **File System Integration**: Temporary file storage in `/tmp` with automatic cleanup
- **Memory Management**: Configurable chunk sizes (default: 10,000 records per batch)

### Data Flow Pattern

```
File Upload → Validation → Staging Table → Deduplication → Production Migration → Audit Log
     ↓              ↓           ↓              ↓                ↓                  ↓
WebSocket      Format      Temp Storage    Unique Key     Live Database      History
Progress       Check       Creation        Generation     Update             Tracking
```

## Security Requirements

### Authentication & Authorization

- **Session Validation**: All import endpoints require active HexTrackr session
- **Role-Based Access**: Import functionality restricted to 'admin' and 'operator' roles
- **API Rate Limiting**: Maximum 3 concurrent imports per user session

### Data Protection

- **Input Sanitization**: All CSV fields sanitized against SQL injection and XSS
- **File Type Validation**: Strict MIME type checking (text/csv, application/csv only)
- **File Size Limits**: 100MB maximum upload size with progress monitoring
- **Temporary File Security**:
  - Files stored in secure temp directory with restrictive permissions (600)
  - Automatic cleanup after processing completion or 24-hour timeout

### Audit Logging

- **Import Events**: Complete audit trail including user, timestamp, record count, success/failure
- **Data Changes**: Snapshot of changes with before/after states in `vulnerability_snapshots`
- **Security Events**: Failed authentication attempts, suspicious file uploads, rate limit violations

## Performance Targets

### Import Performance

- **Small Datasets** (<1,000 records): <5 seconds end-to-end
- **Medium Datasets** (1,000-10,000 records): <30 seconds end-to-end  
- **Large Datasets** (10,000-100,000 records): <5 minutes end-to-end
- **Memory Usage**: <512MB peak memory consumption regardless of file size

### Database Performance

- **Query Response**: <500ms for vulnerability lookups during deduplication
- **Index Utilization**: 95%+ query coverage by existing indexes
- **Concurrent Access**: Support for up to 5 simultaneous read operations during import
- **Transaction Size**: Batch operations in chunks of 1,000 records to prevent lock timeouts

### User Experience

- **Progress Updates**: Real-time progress via WebSocket every 100 processed records
- **Response Time**: Initial upload acknowledgment within 2 seconds
- **Error Reporting**: Detailed error messages with line numbers for validation failures

## Risk Assessment

### Technical Risks

**High Priority:**

1. **Memory Exhaustion**: Large CSV files could exceed available system memory
   - **Mitigation**: Stream processing with configurable chunk sizes
   - **Monitoring**: Memory usage alerts at 80% system capacity

2. **Database Lock Contention**: Long-running imports blocking other operations
   - **Mitigation**: Smaller transaction batches with periodic commits
   - **Monitoring**: Transaction duration alerts above 30 seconds

**Medium Priority:**
3. **Duplicate Detection Accuracy**: False positives/negatives in deduplication logic

- **Mitigation**: Confidence scoring system with manual review queue
- **Testing**: Comprehensive test dataset with known duplicate patterns

4. **CSV Format Variations**: Inconsistent field mappings across different scanners
   - **Mitigation**: Flexible field mapping configuration with validation
   - **Documentation**: Scanner-specific import guides

### Operational Risks

**Medium Priority:**
5. **Scalability Bottleneck**: Sequential processing limits throughput for multiple users

- **Mitigation**: Queue system for multiple concurrent imports
- **Future Enhancement**: Parallel processing capabilities

6. **Data Integrity**: Partial imports due to system failures
   - **Mitigation**: Atomic staging table operations with rollback capability
   - **Recovery**: Automated cleanup and retry mechanisms

### Rollback Procedures

**Import Failure Recovery:**

1. Staging table cleanup (automatic)
2. Temporary file removal (automatic)
3. WebSocket connection cleanup (automatic)
4. User notification of failure cause (automatic)

**Data Corruption Recovery:**

1. Restore from `vulnerability_snapshots` table
2. Rollback to previous daily snapshot
3. Manual data verification and cleanup
4. System integrity check and index rebuild

---

*This research analysis supports the implementation planning for HexTrackr Specification 002: Vulnerability Import System*
